{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#    \n",
    "#    TensorFlow is a versatile framework that can be used for a variety of Machine Learning problems \n",
    "#    besides Deep-Learning. \n",
    "#    This demo implements a classic Machine Learning problem of Text Classification with TensorFlow.\n",
    "#    It uses a technique known as \"bag of words\" which relies on presence of certain words, their frequency \n",
    "#    and their uniqueness to form the features for learning. \n",
    "#    The classifier used is a \"shallow\" neural network with a single hidden layer.\n",
    "#    \n",
    "#    Author: Harshal Patil\n",
    "#    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     load packages\n",
    "import os \n",
    "import zipfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "import time \n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     load the data\n",
    "#    The data is received from http://jmcauley.ucsd.edu/data/amazon/ \n",
    "#    You will need to contact Julian McAuley of UCSD to obtain it\n",
    "\n",
    "#    The data contains a sample of 1,000,000 Amazon product reviews \n",
    "#    There are -\n",
    "#    Unique Review Writers: 267,504\n",
    "#    Unique Products Reviewed: 615,419\n",
    "#    Starting Date: Jan, 1997\n",
    "#    Ending Date: Dec, 2014\n",
    "  \n",
    "#    I have extracted the data from Json structures and loaded into a dataframe\n",
    "#    The dataframe will be read from the pickle\n",
    "pname=\"amazon_reviews_df.pickle\"\n",
    "#fname = os.path.join(os.getcwd(),pname)\n",
    "df = pd.read_pickle(pname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noreviews = len(df)\n",
    "users=len(df[\"reviewerID\"].unique())\n",
    "prods=len(df[\"asin\"].unique())\n",
    "df[\"datevec\"]=pd.to_datetime(df[\"reviewTime\"])\n",
    "syear = min(df[\"datevec\"].apply(lambda x: x.year))\n",
    "smth = min(df[\"datevec\"].apply(lambda x: x.month))\n",
    "eyear = max(df[\"datevec\"].apply(lambda x: x.year))\n",
    "emth = max(df[\"datevec\"].apply(lambda x: x.month))\n",
    "\n",
    "print \"No of Product Reviews, Unique Users, Unique Products, Year, Month To Year, Month\"\n",
    "print [noreviews, users, prods, syear, smth, eyear, emth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    We are interested in predicting a negative review purely based on review headline and review text\n",
    "#    We consider any review that is a 2 star or 1 star as negative and label it as 1 \n",
    "\n",
    "df[\"labelvec\"] = np.vectorize(lambda x: 1 if(x<3.0) else 0)(df[\"overall\"])\n",
    "df[\"textvec\"] = df[\"summary\"] + \" \" + df[\"reviewText\"]\n",
    " \n",
    "\n",
    "#    check proper coding of label from class\n",
    "print \"1. check counts and percentages \\n\"\n",
    "print pd.crosstab(df[\"labelvec\"],df[\"overall\"],margins=True)\n",
    "print \"\\n\"\n",
    "\n",
    "#    check balance between positive and negative cases\n",
    "#    check the percentage of positive cases \n",
    "print pd.crosstab(df[\"labelvec\"],df[\"overall\"],margins=True).apply(lambda x: (x/len(df))*100, axis=1)\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    preprocess strings to normalize and improve model performance\n",
    "#    conver to lowercase, remove punctuations, numbers and extra spaces, tabs etc.\n",
    "#    Stopwords removal at the time of Doc x Term matrix creation\n",
    "if not os.path.exists(\"Amazondocterm.pickle\"):\n",
    "\tt1 = time.time()\n",
    "\tmessage = np.vectorize(lambda x: x.lower())(df[\"textvec\"])\n",
    "\tmessage = np.vectorize(lambda x: re.sub(\"[\"+string.punctuation+\"]\",\" \",x))(message)\n",
    "\tmessage = np.vectorize(lambda x: re.sub(\"[0123456789]\",\" \",x))(message)\n",
    "\tmessage = np.vectorize(lambda x: \" \".join(x.split()))(message)\n",
    "\tt2 = time.time()\n",
    "\n",
    "\tprint \"\\n Text pre-processing time in seconds: \" + str(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    we will create a documentxterm matrix which will be passed to learner\n",
    "#    The counts will be using TF-IDF to differentiate unique terms and add higher weight to them\n",
    "#    you may need to run nltk.download()\n",
    "#    consider only the top 5000 words as features\n",
    "\n",
    "word_features = 5000\n",
    "\n",
    "if not os.path.exists(\"Amazondocterm.pickle\"):\n",
    "\tt1 = time.time()\n",
    "\ttfidfconvert = TfidfVectorizer(tokenizer=nltk.word_tokenize,stop_words=\"english\",max_features=word_features)\n",
    "\ttermdoc = tfidfconvert.fit_transform(message)\n",
    "\tt2=time.time()\n",
    "\n",
    "\tprint \"\\n Doc x Term Matrix pre-processing time in seconds: \" + str(t2-t1)\n",
    "\twith open(\"Amazondocterm.pickle\",\"wb\") as f:\n",
    "\t\tpickle.dump(termdoc,f)\n",
    "else:\n",
    "\twith open(\"Amazondocterm.pickle\",\"rb\") as f:\n",
    "\t\ttermdoc = pickle.load(f)\n",
    "\n",
    "print \"2. Document x Term Matrix size : \"+ str(termdoc.shape[0])+\" rows by \"+ str(termdoc.shape[1])+\" columns \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow = termdoc.shape[0]\n",
    "ncol = termdoc.shape[1]\n",
    "\n",
    "#    divide in training, cross validation and a test set \n",
    "train_set = np.random.choice(nrow,int(round(0.8*nrow)),replace=False)\n",
    "remn_data = np.delete(range(nrow),train_set)\n",
    "cv_set = np.random.choice(remn_data,int(round(0.9*remn_data.shape[0])),replace=False)\n",
    "test_set = np.setdiff1d(remn_data,cv_set)\n",
    "\n",
    "print \"4. Training Set Size : \" + str(len(train_set))\n",
    "print \"5. Cross-Validation Set Size : \" + str(len(cv_set))\n",
    "print \"6. Test Set Size : \" + str(len(test_set))\n",
    "\n",
    "# actual training and test data\n",
    "matrix_train = termdoc[train_set] \n",
    "matrix_cv = termdoc[cv_set] \n",
    "matrix_test = termdoc[test_set] \n",
    "target_train = np.transpose([df[\"labelvec\"][train_set]])\n",
    "target_cv = np.transpose([df[\"labelvec\"][cv_set]])\n",
    "target_test = np.transpose([df[\"labelvec\"][test_set]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\n\\n Establish TensorFlow graph \\n\"\n",
    "#    build tensorflow graph\n",
    "input_layer = word_features\n",
    "hidden_layer = 50 #arbitrary no of units in hidden layer\n",
    "output_layer = 1\n",
    "\n",
    "# DEPLOYMENT CHOICE with Simple Settings\n",
    "# specify device - we will change this to GPU to deploy on specific GPU\n",
    "with tf.device('/gpu:0'):\n",
    "#with tf.device('/cpu:0'):\n",
    "    \n",
    "    with tf.name_scope('input') as scope:\n",
    "        inp = tf.placeholder(tf.float32, [None, input_layer],name=\"inp\")\n",
    "        hidden_w = tf.Variable(tf.random_normal(shape=[input_layer,hidden_layer]),name=\"hidden_w\")\n",
    "        hidden_b = tf.Variable(tf.random_normal(shape=[hidden_layer]),name=\"hidden_b\")\n",
    "        inp_model = tf.add(tf.matmul(inp,hidden_w),hidden_b)\n",
    "        \n",
    "    #init all weights to random\n",
    "    with tf.name_scope('hidden') as scope:\n",
    "        #hidden_model = tf.nn.relu(inp_model)\n",
    "        hidden_model = tf.sigmoid(inp_model)\n",
    "    \n",
    "    with tf.name_scope('output') as scope:\n",
    "        op = tf.placeholder(tf.float32, [None, output_layer],name=\"op\")\n",
    "        op_w = tf.Variable(tf.random_normal(shape=[hidden_layer,output_layer]),name=\"op_w\")\n",
    "        op_b = tf.Variable(tf.random_normal(shape=[output_layer]),name=\"op_b\")\n",
    "        op_model = tf.add(tf.matmul(hidden_model,op_w),op_b)\n",
    "        predict = tf.round(tf.sigmoid(op_model))\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predict,op), tf.float32))\n",
    "\n",
    "    with tf.name_scope('cost') as scope:\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=op_model,labels=op))\n",
    "    \n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0025).minimize(cost)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0025).minimize(cost)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# LOG DEVICE PLACEMENTs - show that the graph is deployed to CPU or GPU\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "#sess = tf.Session()\n",
    "\n",
    "# create initialized variables\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    training the model - we go through a number of random batches to train\n",
    "batch=50000\n",
    "rounds = 1000\n",
    "# record at these iterations and print\n",
    "recordfreq = range(0,10,1) + range(10,100,10) + range(100,(rounds+100),100)\n",
    "printfreq = 100\n",
    "\n",
    "# test accuracy in training loop is calculated on cross-validation \n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "i_data = []\n",
    "runtime = []\n",
    "i=0\n",
    "\n",
    "time_start = time.time()\n",
    "for i in range(rounds):\n",
    "#        Train\n",
    "    rand_index = np.random.choice(matrix_train.shape[0], size=batch)\n",
    "    rand_x = matrix_train[rand_index].todense() \n",
    "    rand_y = target_train[rand_index]\n",
    "    sess.run(optimizer, feed_dict={inp: rand_x, op: rand_y})\n",
    "    \n",
    "    # Only record loss and accuracy every 100 generations\n",
    "    if (i) in recordfreq:\n",
    "        i_data.append(i+1)\n",
    "        t1 = time.time()\n",
    "        train_loss_temp = sess.run(cost, feed_dict={inp: rand_x, op: rand_y})\n",
    "        t2=time.time()\n",
    "        train_loss.append(train_loss_temp)\n",
    "        runtime.append(t2-t1)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        test_loss_temp = sess.run(cost, feed_dict={inp: matrix_cv.todense(), op: target_cv})\n",
    "        t2=time.time()\n",
    "        test_loss.append(test_loss_temp)\n",
    "        runtime.append(t2-t1)\n",
    "         \n",
    "        t1 = time.time()\n",
    "        train_acc_temp = sess.run(accuracy, feed_dict={inp: rand_x, op: rand_y})\n",
    "        t2=time.time()\n",
    "        train_acc.append(train_acc_temp)\n",
    "        runtime.append(t2-t1)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        test_acc_temp = sess.run(accuracy, feed_dict={inp: matrix_cv.todense(), op: target_cv})\n",
    "        t2=time.time()\n",
    "        test_acc.append(test_acc_temp)\n",
    "        runtime.append(t2-t1)\n",
    "    \n",
    "    if (i) in recordfreq:\n",
    "        acc_and_loss = [i+1, train_loss_temp, test_loss_temp, train_acc_temp, test_acc_temp]\n",
    "        acc_and_loss = [np.round(x,2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train Loss (Validation Loss): {:.2f} ({:.2f}). Train Acc (Validation Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))\n",
    "\n",
    "time_end = time.time()\n",
    "print \"\\n 6. Training time in seconds (total) \" + str(time_end - time_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the graph to a log file to see via Tensorboard\n",
    "writer = tf.summary.FileWriter('tflogs', sess.graph)\n",
    "print sess.run(accuracy,feed_dict={inp: matrix_test.todense(), op: target_test})\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"\\n 8. Loss Function\"\n",
    "plt.plot(i_data,train_loss,label=\"Training Set\")\n",
    "plt.plot(i_data,test_loss,label=\"Validation Set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"9. Accuracy\"\n",
    "plt.plot(i_data,train_acc,label=\"Training Set\")\n",
    "plt.plot(i_data,test_acc,label=\"Validation Set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"10. Final Model Evaluation on Test Set\"\n",
    "\n",
    "predictions  = np.vectorize(lambda x: 1 if x>0.5 else 0)(sess.run(predict, feed_dict={inp: matrix_test.todense(), op: target_test}))\n",
    "\n",
    "testdf = pd.DataFrame(predictions,columns=[\"predictions\"])\n",
    "testdf[\"target\"] = target_test\n",
    "posdf = testdf[testdf[\"target\"]==1]\n",
    "\n",
    "true_pos = float(sum(posdf[\"predictions\"]))\n",
    "tot_pred = sum(testdf[\"predictions\"])\n",
    "all_pos = len(posdf)\n",
    "\n",
    "prec = round(true_pos/tot_pred,2)\n",
    "rec = round(true_pos/all_pos,2)\n",
    "fscore = 2*((prec*rec)/(prec+rec))\n",
    "\n",
    "print \"Accuracy from TensorFlow : \" + str(sess.run(accuracy, feed_dict={inp: matrix_test.todense(), op: target_test}))\n",
    "print \"Precision %: \" + str(prec*100)\n",
    "print \"Recall %: \" + str(rec*100) \n",
    "print \"F-Score: (max is 1) \" + str(fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
